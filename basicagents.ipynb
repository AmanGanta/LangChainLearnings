{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PULLING PREDIFIED PROMPTS FROM HUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "p1=hub.pull(\"hwchase17/openai-functions-agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')),\n",
       " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATING TOOLS EXAMPLE WIKIPEDIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wikipedia'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.tools.wikipedia.tool import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "ap=WikipediaAPIWrapper(top_k_results=2,doc_content_chars_max=300)\n",
    "t1=WikipediaQueryRun(api_wrapper=ap)\n",
    "t1.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FOR GOOGLE \n",
    "To get your GOOGLE_CSE_ID and GOOGLE_API_KEY, follow these steps:\n",
    "\n",
    "Google Custom Search Engine (CSE) ID:\n",
    "\n",
    "Go to the Google Custom Search Engine website.\n",
    "Click on \"Add\" to create a new search engine.\n",
    "Fill in the required fields (e.g., Sites to Search).\n",
    "Once created, go to the CSE control panel and find the \"Search engine ID\". This is your GOOGLE_CSE_ID.\n",
    "Google API Key:\n",
    "\n",
    "Go to the Google Cloud Console.\n",
    "Create a new project or select an existing one.\n",
    "Navigate to the \"APIs & Services\" > \"Credentials\" page.\n",
    "Click \"Create credentials\" and select \"API key\".\n",
    "Copy the generated API key. This is your GOOGLE_API_KEY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GOOGLE_CSE_ID\"] = \"f032bbce34948424bf4\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyArSGuRGoUN44MHzK5QP_NcTlJoTTXuwvXKY\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regular Method                                \n",
    "google_search = Tool(                         \n",
    "    name=\"google_search\",                        \n",
    "    func=google_search_tool.run,                    \n",
    "    description=\"Use Google Search to answer questions.\"                            \n",
    ")\n",
    "#####regular method caused error so we have updated to structural tool below                                \n",
    "\n",
    "ToolException: Too many arguments to single-input tool google_search.               \n",
    "                Consider using StructuredTool instead. Args: [{'tags': ['time in India now'], 'max_concurrency': 2}, ['time in India now']]     \n",
    "Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...\n",
    "ac.invoke( {'input': ['what is time in india now']})                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StructuredTool and args_schema: Simple Explanation                  \n",
    "What is StructuredTool?                   \n",
    "StructuredTool: It's a way to define tools that expect inputs to be in a specific format.               \n",
    "What is args_schema?                     \n",
    "args_schema: It's a blueprint that describes the exact format and type of inputs that a tool expects. This blueprint is defined using pydantic, a library for data validation.                     \n",
    "Why Use StructuredTool and args_schema?                     \n",
    "Validation: Ensures that the inputs to the tool are correct and in the expected format before running the tool's function.           \n",
    "Clarity: Makes it clear what kind of inputs the tool needs.               \n",
    "Error Handling: Provides clear error messages if the inputs are wrong, making it easier to fix problems.              \n",
    "How It Works                \n",
    "Define the Input Schema: You create a pydantic model to specify what inputs your tool needs.                    \n",
    "Create the StructuredTool: You use this schema to create a StructuredTool, linking the tool to its expected inputs.              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import GoogleSearchAPIWrapper\n",
    "from langchain_core.tools import StructuredTool\n",
    "from pydantic import BaseModel\n",
    "\n",
    "search = GoogleSearchAPIWrapper(k=2)\n",
    "\n",
    "class GoogleSearchArgs(BaseModel):\n",
    "    query: str\n",
    "    \n",
    "t2 = StructuredTool(\n",
    "    name=\"google_search\",\n",
    "    description=\"Search Google and return the first 2 results.\",\n",
    "    func=search.run,\n",
    "    args_schema=GoogleSearchArgs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Child's First Name (Type or print) lb. Middle Name. BARACK. HUSSEIN. CERTIFICATE OF LIVE BIRTH. FILE 151. NUMBER le. DEPARTMENT OF HEALTH. 61. 10641. Last Name. As a member of the Democratic Party, he was the first African-American president in U.S. history. Obama previously served as a U.S. senator representing\\xa0...\""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.run(\"Obama's first name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FOR WEBSITE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "l=WebBaseLoader(\"https://medium.com/\")\n",
    "dc=l.load()\n",
    "doc=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200).split_documents(dc)\n",
    "db=FAISS.from_documents(dc,OpenAIEmbeddings())\n",
    "re=db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "t3=create_retriever_tool(re,\"medium-search\",\"Search in medium\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arxiv Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_community.utilities import ArxivAPIWrapper\n",
    "from langchain_community.tools import ArxivQueryRun\n",
    "ar_wr=ArxivAPIWrapper(top_k_results=2,doc_content_chars_max=200)\n",
    "t4=ArxivQueryRun(api_wrapper=ar_wr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from 'c:\\\\Users\\\\amang\\\\Downloads\\\\proj celeb search by krish naik\\\\venv\\\\lib\\\\site-packages\\\\wikipedia\\\\__init__.py'>, top_k_results=2, lang='en', load_all_available_meta=False, doc_content_chars_max=300)),\n",
       " StructuredTool(name='google_search', description='Search Google and return the first 2 results.', args_schema=<class '__main__.GoogleSearchArgs'>, func=<bound method GoogleSearchAPIWrapper.run of GoogleSearchAPIWrapper(search_engine=<googleapiclient.discovery.Resource object at 0x000002B5BE000BB0>, google_api_key='AIzaSyArSGuRGoUNMHzK5QP_NcTlJoTTXuwvXKY', google_cse_id='f032bbce349484bf4', k=2, siterestrict=False)>),\n",
       " Tool(name='medium-search', description='Search in medium', args_schema=<class 'langchain_core.tools.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x000002B5EEFAA8C0>, retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000002B5BE0028C0>), document_prompt=PromptTemplate(input_variables=['page_content'], template='{page_content}'), document_separator='\\n\\n'), coroutine=functools.partial(<function _aget_relevant_documents at 0x000002B5EEFAA950>, retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000002B5BE0028C0>), document_prompt=PromptTemplate(input_variables=['page_content'], template='{page_content}'), document_separator='\\n\\n')),\n",
       " ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=2, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=200))]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools=[t1,t2,t3,t4]\n",
    "tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "llm=ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_openai_tools_agent\n",
    "agent=create_openai_tools_agent(llm,tools,p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "ac=AgentExecutor(agent=agent,tools=tools,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wikipedia` with `{'query': 'LLM Fine-Tuning For Text Classification Using QLoRA'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mPage: Large language model\n",
      "Summary: A large language model (LLM) is a computational model notable for its ability to achieve general-purpose language generation and other natural language processing tasks such as classification. Based on language models, LLMs acquire these abilities by learning stat\u001b[0m\u001b[32;1m\u001b[1;3mLLM Fine-Tuning for Text Classification using QLoRA is a method that involves using a large language model (LLM) for text classification tasks. LLMs are computational models known for their ability to perform various natural language processing tasks, including classification. QLoRA is likely a specific technique or approach used in fine-tuning LLMs for text classification purposes.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'LLM Fine-Tuning For Text Classification Using QLoRA', 'output': 'LLM Fine-Tuning for Text Classification using QLoRA is a method that involves using a large language model (LLM) for text classification tasks. LLMs are computational models known for their ability to perform various natural language processing tasks, including classification. QLoRA is likely a specific technique or approach used in fine-tuning LLMs for text classification purposes.'}\n"
     ]
    }
   ],
   "source": [
    "q = \"LLM Fine-Tuning For Text Classification Using QLoRA\"\n",
    "result = ac.invoke({\"input\": q})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
